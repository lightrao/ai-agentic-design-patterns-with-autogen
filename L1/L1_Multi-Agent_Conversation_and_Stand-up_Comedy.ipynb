{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lightrao/ai-agentic-design-patterns-with-autogen/blob/main/L1/L1_Multi-Agent_Conversation_and_Stand-up_Comedy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a81456dd",
      "metadata": {
        "id": "a81456dd"
      },
      "source": [
        "# Lesson 1: Multi-Agent Conversation and Stand-up Comedy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4693467e",
      "metadata": {
        "id": "4693467e"
      },
      "source": [
        "Welcome to Lesson 1.\n",
        "\n",
        "To access the `requirements.txt` file and the`utils` modules, please go to the `File` menu and select`Open...`.\n",
        "\n",
        "I hope you enjoy this course!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "742cf649",
      "metadata": {
        "id": "742cf649"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "366d40f0",
      "metadata": {
        "collapsed": true,
        "id": "366d40f0",
        "outputId": "b4e536f9-d2cd-467c-83ef-f79f619a84a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyautogen\n",
            "  Downloading pyautogen-0.8.6-py3-none-any.whl.metadata (35 kB)\n",
            "Collecting chess\n",
            "  Downloading chess-1.11.2.tar.gz (6.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (0.2.55)\n",
            "Requirement already satisfied: anyio<5.0.0,>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from pyautogen) (4.9.0)\n",
            "Collecting asyncer==0.0.8 (from pyautogen)\n",
            "  Downloading asyncer-0.0.8-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting diskcache (from pyautogen)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting docker (from pyautogen)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from pyautogen) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pyautogen) (24.2)\n",
            "Requirement already satisfied: pydantic<3,>=2.6.1 in /usr/local/lib/python3.11/dist-packages (from pyautogen) (2.11.2)\n",
            "Collecting python-dotenv (from pyautogen)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from pyautogen) (3.0.1)\n",
            "Collecting tiktoken (from pyautogen)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.3.7)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.17.9)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.13.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=3.0.0->pyautogen) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=3.0.0->pyautogen) (1.3.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=3.0.0->pyautogen) (4.13.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.6)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.28.1->pyautogen) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.28.1->pyautogen) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.28.1->pyautogen) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6.1->pyautogen) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6.1->pyautogen) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6.1->pyautogen) (0.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2.3.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->pyautogen) (2024.11.6)\n",
            "Downloading pyautogen-0.8.6-py3-none-any.whl (734 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m734.2/734.2 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asyncer-0.0.8-py3-none-any.whl (9.2 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m834.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: chess\n",
            "  Building wheel for chess (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for chess: filename=chess-1.11.2-py3-none-any.whl size=147775 sha256=7b34c6f48c81ef399a11714a0f352c5663f0fac399ebb5c2f9855e7e189f3112\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/5d/5c/59a62d8a695285e59ec9c1f66add6f8a9ac4152499a2be0113\n",
            "Successfully built chess\n",
            "Installing collected packages: python-dotenv, diskcache, chess, tiktoken, docker, asyncer, pyautogen\n",
            "Successfully installed asyncer-0.0.8 chess-1.11.2 diskcache-5.6.3 docker-7.1.0 pyautogen-0.8.6 python-dotenv-1.1.0 tiktoken-0.9.0\n"
          ]
        }
      ],
      "source": [
        "%pip install pyautogen chess numpy matplotlib pandas yfinance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "04d006c1-22fa-40ea-b3e0-d543142e0788",
      "metadata": {
        "height": 64,
        "id": "04d006c1-22fa-40ea-b3e0-d543142e0788"
      },
      "outputs": [],
      "source": [
        "# from utils import get_openai_api_key\n",
        "# OPENAI_API_KEY = get_openai_api_key()\n",
        "\n",
        "import os\n",
        "\n",
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "llm_config = {\"model\": \"gpt-3.5-turbo\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "116a1c4d",
      "metadata": {
        "id": "116a1c4d"
      },
      "source": [
        "## Define an AutoGen agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6fb8c441-c58c-41a8-a54b-5c387afceac5",
      "metadata": {
        "height": 132,
        "id": "6fb8c441-c58c-41a8-a54b-5c387afceac5"
      },
      "outputs": [],
      "source": [
        "from autogen import ConversableAgent\n",
        "\n",
        "agent = ConversableAgent(\n",
        "    name=\"chatbot\",\n",
        "    llm_config=llm_config,\n",
        "    human_input_mode=\"NEVER\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "47886b5f-fc7c-431a-8036-cff6e88f85c6",
      "metadata": {
        "height": 95,
        "id": "47886b5f-fc7c-431a-8036-cff6e88f85c6",
        "outputId": "a762d01f-470a-42a5-e6ba-fc732d4d9cd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why was the math book sad? Because it had too many problems.\n"
          ]
        }
      ],
      "source": [
        "reply = agent.generate_reply(\n",
        "    messages=[{\"content\": \"Tell me a joke.\", \"role\": \"user\"}]\n",
        ")\n",
        "print(reply)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "67f626e9-4cec-40c1-abde-2eff1252b848",
      "metadata": {
        "height": 95,
        "id": "67f626e9-4cec-40c1-abde-2eff1252b848",
        "outputId": "008eba47-eff5-4ef8-831c-45208d9059e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Of course! Just let me know which joke you'd like me to repeat for you.\n"
          ]
        }
      ],
      "source": [
        "reply = agent.generate_reply(\n",
        "    messages=[{\"content\": \"Repeat the joke.\", \"role\": \"user\"}]\n",
        ")\n",
        "print(reply)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c98a301",
      "metadata": {
        "id": "8c98a301"
      },
      "source": [
        "## Conversation\n",
        "\n",
        "Setting up a conversation between two agents, Cathy and Joe, where the memory of their interactions is retained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8f109dcb-824e-40d7-8e86-efee42b75f3c",
      "metadata": {
        "height": 299,
        "id": "8f109dcb-824e-40d7-8e86-efee42b75f3c"
      },
      "outputs": [],
      "source": [
        "cathy = ConversableAgent(\n",
        "    name=\"cathy\",\n",
        "    system_message=\n",
        "    \"Your name is Cathy and you are a stand-up comedian.\",\n",
        "    llm_config=llm_config,\n",
        "    human_input_mode=\"NEVER\",\n",
        ")\n",
        "\n",
        "joe = ConversableAgent(\n",
        "    name=\"joe\",\n",
        "    system_message=\n",
        "    \"Your name is Joe and you are a stand-up comedian. \"\n",
        "    \"Start the next joke from the punchline of the previous joke.\",\n",
        "    llm_config=llm_config,\n",
        "    human_input_mode=\"NEVER\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43f71a61",
      "metadata": {
        "id": "43f71a61"
      },
      "source": [
        "**Note**: You might get a slightly different response (set of jokes) than what is shown in the video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "46a1c6f6-687e-40de-8819-374201cfed9f",
      "metadata": {
        "height": 112,
        "id": "46a1c6f6-687e-40de-8819-374201cfed9f",
        "outputId": "132d4e95-d0b5-4fff-d958-9c00e57c5731",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "joe (to cathy):\n",
            "\n",
            "I'm Joe. Cathy, let's keep the jokes rolling.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "cathy (to joe):\n",
            "\n",
            "Sure thing, Joe! What do you call a fish wearing a crown?\n",
            "\n",
            "A kingfish!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "joe (to cathy):\n",
            "\n",
            "Why did the fish blush?\n",
            "\n",
            "Because it saw the ocean's bottom!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "cathy (to joe):\n",
            "\n",
            "Haha, nice one, Joe! Here's another one for you: Why couldn't the bicycle stand up by itself?\n",
            "\n",
            "Because it was two tired!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> TERMINATING RUN (2a2ab203-1ea9-4d0d-821b-e86b70d481bd): Maximum turns (2) reached\n"
          ]
        }
      ],
      "source": [
        "chat_result = joe.initiate_chat(\n",
        "    recipient=cathy,\n",
        "    message=\"I'm Joe. Cathy, let's keep the jokes rolling.\",\n",
        "    max_turns=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78edc810",
      "metadata": {
        "id": "78edc810"
      },
      "source": [
        "## Print some results\n",
        "\n",
        "You can print out:\n",
        "\n",
        "1. Chat history\n",
        "2. Cost\n",
        "3. Summary of the conversation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "1169ea24-eadd-4909-8d56-9b7ec5677c66",
      "metadata": {
        "height": 64,
        "id": "1169ea24-eadd-4909-8d56-9b7ec5677c66",
        "outputId": "35d658e5-e527-4f0e-9ce3-23ac85650d27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'content': \"I'm Joe. Cathy, let's keep the jokes rolling.\",\n",
            "  'name': 'joe',\n",
            "  'role': 'assistant'},\n",
            " {'content': 'Sure thing, Joe! What do you call a fish wearing a crown?\\n'\n",
            "             '\\n'\n",
            "             'A kingfish!',\n",
            "  'name': 'cathy',\n",
            "  'role': 'user'},\n",
            " {'content': \"Why did the fish blush?\\n\\nBecause it saw the ocean's bottom!\",\n",
            "  'name': 'joe',\n",
            "  'role': 'assistant'},\n",
            " {'content': \"Haha, nice one, Joe! Here's another one for you: Why couldn't \"\n",
            "             'the bicycle stand up by itself?\\n'\n",
            "             '\\n'\n",
            "             'Because it was two tired!',\n",
            "  'name': 'cathy',\n",
            "  'role': 'user'}]\n"
          ]
        }
      ],
      "source": [
        "import pprint\n",
        "\n",
        "pprint.pprint(chat_result.chat_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "550267b6-3652-40dc-9997-c5401f6d4c47",
      "metadata": {
        "height": 30,
        "id": "550267b6-3652-40dc-9997-c5401f6d4c47",
        "outputId": "aa18c99c-0b1c-42d1-c1c2-979ddced32fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'usage_excluding_cached_inference': {'gpt-3.5-turbo-0125': {'completion_tokens': 67,\n",
            "                                                             'cost': 0.000199,\n",
            "                                                             'prompt_tokens': 197,\n",
            "                                                             'total_tokens': 264},\n",
            "                                      'total_cost': 0.000199},\n",
            " 'usage_including_cached_inference': {'gpt-3.5-turbo-0125': {'completion_tokens': 67,\n",
            "                                                             'cost': 0.000199,\n",
            "                                                             'prompt_tokens': 197,\n",
            "                                                             'total_tokens': 264},\n",
            "                                      'total_cost': 0.000199}}\n"
          ]
        }
      ],
      "source": [
        "pprint.pprint(chat_result.cost)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "dfcf468e-d217-4731-8cb4-3485377230f1",
      "metadata": {
        "height": 30,
        "id": "dfcf468e-d217-4731-8cb4-3485377230f1",
        "outputId": "12d73758-1388-45a6-e1d2-239f2b892d24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(\"Haha, nice one, Joe! Here's another one for you: Why couldn't the bicycle \"\n",
            " 'stand up by itself?\\n'\n",
            " '\\n'\n",
            " 'Because it was two tired!')\n"
          ]
        }
      ],
      "source": [
        "pprint.pprint(chat_result.summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba8c6cf8",
      "metadata": {
        "id": "ba8c6cf8"
      },
      "source": [
        "## Get a better summary of the conversation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "c1a8fef1-8030-4652-a2d2-1648834f62c2",
      "metadata": {
        "height": 146,
        "id": "c1a8fef1-8030-4652-a2d2-1648834f62c2",
        "outputId": "a7fcb873-971a-4238-c235-eff7ba415ef4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "joe (to cathy):\n",
            "\n",
            "I'm Joe. Cathy, let's keep the jokes rolling.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "cathy (to joe):\n",
            "\n",
            "Sure thing, Joe! What do you call a fish wearing a crown?\n",
            "\n",
            "A kingfish!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "joe (to cathy):\n",
            "\n",
            "Why did the fish blush?\n",
            "\n",
            "Because it saw the ocean's bottom!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "cathy (to joe):\n",
            "\n",
            "Haha, nice one, Joe! Here's another one for you: Why couldn't the bicycle stand up by itself?\n",
            "\n",
            "Because it was two tired!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> TERMINATING RUN (51714028-9e8b-4fef-8846-ae710c788f89): Maximum turns (2) reached\n"
          ]
        }
      ],
      "source": [
        "chat_result = joe.initiate_chat(\n",
        "    cathy,\n",
        "    message=\"I'm Joe. Cathy, let's keep the jokes rolling.\",\n",
        "    max_turns=2,\n",
        "    summary_method=\"reflection_with_llm\",\n",
        "    summary_prompt=\"Summarize the conversation\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b042de62-bc49-49ee-99f2-4f972e23670b",
      "metadata": {
        "height": 30,
        "id": "b042de62-bc49-49ee-99f2-4f972e23670b"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(chat_result.summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "300525bd",
      "metadata": {
        "id": "300525bd"
      },
      "source": [
        "## Chat Termination\n",
        "\n",
        "Chat can be terminated using a termination conditions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "044dfd61-7f1d-46d8-9e28-4b2601b43d70",
      "metadata": {
        "height": 350,
        "id": "044dfd61-7f1d-46d8-9e28-4b2601b43d70"
      },
      "outputs": [],
      "source": [
        "cathy = ConversableAgent(\n",
        "    name=\"cathy\",\n",
        "    system_message=\n",
        "    \"Your name is Cathy and you are a stand-up comedian. \"\n",
        "    \"When you're ready to end the conversation, say 'I gotta go'.\",\n",
        "    llm_config=llm_config,\n",
        "    human_input_mode=\"NEVER\",\n",
        "    is_termination_msg=lambda msg: \"I gotta go\" in msg[\"content\"],\n",
        ")\n",
        "\n",
        "joe = ConversableAgent(\n",
        "    name=\"joe\",\n",
        "    system_message=\n",
        "    \"Your name is Joe and you are a stand-up comedian. \"\n",
        "    \"When you're ready to end the conversation, say 'I gotta go'.\",\n",
        "    llm_config=llm_config,\n",
        "    human_input_mode=\"NEVER\",\n",
        "    is_termination_msg=lambda msg: \"I gotta go\" in msg[\"content\"] or \"Goodbye\" in msg[\"content\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "bc49d959-1025-4709-8866-9d4035eaeae7",
      "metadata": {
        "height": 95,
        "id": "bc49d959-1025-4709-8866-9d4035eaeae7",
        "outputId": "abb20b8c-a04c-4778-b47a-72ceab2dc15b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "joe (to cathy):\n",
            "\n",
            "I'm Joe. Cathy, let's keep the jokes rolling.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "cathy (to joe):\n",
            "\n",
            "Hey Joe! Glad to have another audience member ready for some laughs. What do you call a fake noodle? An impasta!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "joe (to cathy):\n",
            "\n",
            "Haha, that's a classic one, Cathy! Here's another one for you: Why did the math book look sad? Because it had too many problems.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "cathy (to joe):\n",
            "\n",
            "Great one, Joe! How about this one: Why couldn't the bicycle find its way home? Because it lost its bearings!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "joe (to cathy):\n",
            "\n",
            "Haha, I love it, Cathy! You've got some good jokes up your sleeve. Here's one more for you: Why did the scarecrow win an award? Because he was outstanding in his field!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "cathy (to joe):\n",
            "\n",
            "Haha, that's a good one, Joe! You've definitely got a talent for jokes too. Thanks for the laughs!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "joe (to cathy):\n",
            "\n",
            "You're welcome, Cathy! I had a great time sharing jokes with you. Keep spreading the laughter! I gotta go.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> TERMINATING RUN (dd1ef721-2fee-45f6-bd98-aaf36e81a60a): Termination message condition on agent 'cathy' met\n"
          ]
        }
      ],
      "source": [
        "chat_result = joe.initiate_chat(\n",
        "    recipient=cathy,\n",
        "    message=\"I'm Joe. Cathy, let's keep the jokes rolling.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "846eccbd-efd1-464b-9385-279c19b17c1d",
      "metadata": {
        "height": 44,
        "id": "846eccbd-efd1-464b-9385-279c19b17c1d",
        "outputId": "791ab2f9-fcd0-4de4-cf2a-60f607feadec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cathy (to joe):\n",
            "\n",
            "What's last joke we talked about?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "joe (to cathy):\n",
            "\n",
            "The last joke we talked about was: Why did the scarecrow win an award? Because he was outstanding in his field!\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "cathy (to joe):\n",
            "\n",
            "Haha, thanks for the reminder, Joe! That's a great one. It was nice chatting with you. Have a wonderful day! If you ever need a good laugh, you know where to find me. Take care! I gotta go.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            ">>>>>>>> TERMINATING RUN (efa12dd6-7da8-4690-b8b6-1669094cc844): Termination message condition on agent 'joe' met\n"
          ]
        }
      ],
      "source": [
        "cathy.send(message=\"What's last joke we talked about?\", recipient=joe)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}